---
title: "Assessing the replicability of item-dropping decisions based on Cronbach's-alpha-if-item-removed"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# TODO

- NA

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
```

```{r}

# dependencies
library(tidyverse)
library(knitr)
library(kableExtra)
library(psych)
library(lme4)
library(sjPlot)
library(ggstance)
library(marginaleffects)
library(janitor)
library(boot)
library(Cairo)
library(ggeffects)
library(ggExtra)

# functions
# rounds all numeric variables in a dataframe to the desired number of places. Non-numeric variables will be ignored.
round_df <- function(df, digits) {
  mutate_if(df, is.numeric, janitor::round_half_up, digits = 2)
}

```

# Data

```{r}

data_nested <- read_rds("../../../data/processed/bainbridge et al/data_nested.rds")

data_scale_names <- read_csv("../../../data/processed/bainbridge et al/scale abbreviations and full names.csv") |>
  mutate(scale = ifelse(is.na(subscale), scale, paste(scale, subscale, sep = "_"))) |>
  select(scale, scale_name)

data_drop_decisions_no_data <- read_rds("../../../data/processed/bainbridge et al/data_drop_decisions_no_data.rds") |>
  rename(sample_size = subset_n_per_split) |>
  mutate(sample_size                   = as.factor(sample_size),
         match_item_if_dropped         = as.factor(match_item_if_dropped),
         match_item_if_dropped_or_none = as.factor(match_item_if_dropped_or_none)) |>
  left_join(data_scale_names, by = "scale")

```

# Full sample metrics

These are already established scales, many of which are well known, with a reasonable range of Cronbach's $\alpha$ values when calculated in a large sample. The subsequent assessment of the replicability of item-dropping recommendations is therefore likely generalizable. 

```{r}

cronbachs_alpha <- function(data){
  psych::alpha(data)$total["raw_alpha"]
}

results_overall <- data_nested |> 
  mutate(results = map(data, cronbachs_alpha)) |>
  unnest(results) |> 
  mutate(n = map(data, nrow)) |>
  unnest(n) |>
  left_join(data_scale_names, by = "scale")

results_overall |>
  select(scale_name, alpha = raw_alpha, n) |>
  round_df(2) |>
  kable() |>
  kable_classic(full_width = FALSE)

results_overall |>
  select(scale_name, alpha = raw_alpha, n) |>
  summarize(k_scales       = n(),
            alpha_min      = min(alpha),
            alpha_max      = max(alpha)) |>
  round_df(2) |>
  kable() |>
  kable_classic(full_width = FALSE)

weighted_mean <- janitor::round_half_up(weighted.mean(results_overall$raw_alpha, results_overall$n), 2)

weighted_sd <- janitor::round_half_up(sqrt(Hmisc::wtd.var(results_overall$raw_alpha, results_overall$n)), 2)

```

Weighted mean $\alpha$ = `r weighted_mean`, weighted SD =  `r weighted_sd`.

# Frequency of item drop recommendations

```{r fig.height=12, fig.width=14}

data_n_items <- data_nested |>
  mutate(n_items_in_scale = as.numeric(map(data, ncol))) |>
  select(-data)

data_drop_decisions_no_data |>
  distinct(scale, item_to_drop_a) |>
  count(scale) |>
  rename(n_distinct_items_with_drop_recommendations = n) |>
  left_join(data_n_items, by = "scale") |>
  kable() |>
  kable_classic(full_width = FALSE)

data_drop_decisions_no_data |>
  distinct(scale, scale_name)

p_freq <- 
  data_drop_decisions_no_data |>
  count(scale_name, item_to_drop_a) |>
  arrange(scale_name, desc(n)) |>
  group_by(scale_name) |>
  mutate(item_rank = paste("ranked_item_", row_number(), sep = ""),
         item_rank = fct_reorder(item_rank, n, .desc = TRUE)) |>
  ungroup() |>
  select(-item_to_drop_a) |>
  #drop_na() |>
  ggplot(aes(item_rank, n)) +
  geom_bar(stat = "identity") +
  scale_x_discrete(labels = seq(from = 1, to = max(data_n_items$n_items_in_scale), by = 1)) +
  facet_wrap(~ scale_name, ncol = 3) +
  xlab("Item ranked by number of drop recommendations") +
  ylab("Number of drop recommendations") +
  theme_light()

p_freq

ggsave(filename  = "../../../communication/plots/plot_drop_recommendation_frequencies_bainbridge_et_al.pdf",
       plot      = p_freq,
       device    = "pdf",
       units     = "in",
       width     = 14,
       height    = 12,
       limitsize = TRUE)

```

# Replication rate of item-dropping recommendations

Using multilevel logistic models

Strategy 1: Drop one item based on max Cronbach's-$\alpha$-if-item-removed. I.e, item dropping was mandatory, even if the alpha in the retained items was lower than the full-scale $\alpha$. Recommendation in each sample could be item 1...item N.

Strategy 2: Drop an item based on max Cronbach's-$\alpha$-if-item-removed if it is higher than the full-scale $\alpha$. I.e, item dropping was not mandatory if it didn't improve $\alpha$. Recommendation in each sample could be item 1...item N or no item dropped.

```{r}

dat <- data_drop_decisions_no_data |>
  select(scale_name, replication, sample_size, match_item_if_dropped, match_item_if_dropped_or_none, sample_size) |>
  pivot_longer(names_to = "strategy",
               values_to = "replicated",
               cols = c(match_item_if_dropped, match_item_if_dropped_or_none))

# fit model
fit <- 
  glmer(replicated ~ 1 + sample_size * strategy + (1 | scale_name),
        family = binomial(link = "logit"),
        data = dat)

# results table
tab_model(fit)

# marginal means
results_estimates <- fit |>
  marginalmeans(variables = c("sample_size", "strategy"), cross = TRUE) |>
  select(sample_size, strategy, marginalmean, ci_lower = conf.low, ci_upper = conf.high)

```

## Fixed effects

### Plot

```{r fig.height=4, fig.width=6}

plot_fe <- 
  ggplot(results_estimates, aes(marginalmean, sample_size, color = strategy)) +
  geom_linerangeh(aes(xmin = ci_lower, xmax = ci_upper), position = position_dodge(width = 0.5)) +
  geom_point(size = 2.5, shape = 15, position = position_dodge(width = 0.5)) +
  scale_color_viridis_d(begin = 0.3, end = 0.7,
                        labels = c("Max \u03B1 if item removed",
                                   "Max \u03B1 if item removed<br/>if it improves \u03B1 compared to full scale"),
                        name = "Item dropping strategy") +
  scale_x_continuous(
    #breaks = c(0, .25, .5, .75, 1), 
    breaks = c(0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0), 
    #labels = c("0.00<br/>(Worse)", "0.25", "0.50", "0.75", "1.00<br/>(Better)"),
    #labels = c("0.00", "0.25", "0.50", "0.75", "1.00"),
    #labels = c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0"),
    limits = c(0,1)
  ) +
  labs(x = "Replication rate",
       y = "Sample size") +
  mdthemes::md_theme_linedraw() +
  theme(legend.position = c(0.73, 0.18), # "right",
        legend.key.height = unit(0.05, 'npc'),
        panel.grid.minor.x = element_blank(),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  guides(color = guide_legend(reverse = TRUE))

plot_fe

ggsave(filename  = "../../../communication/plots/plot_replication_rate_bainbridge_et_al.pdf",
       plot      = plot_fe,
       device    = cairo_pdf, # for greek letters
       units     = "in",
       width     = 6,
       height    = 4,
       limitsize = TRUE)

```

### Table

```{r}

results_estimates |>
  round_df(2) |>
  arrange(sample_size, strategy) |>
  select(strategy, sample_size, marginalmean, ci_lower, ci_upper) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Random effects

For sample_size = 100 (which has data for the most scales) and the Max $a$ if item removed strategy. 

### Plot

```{r fig.height=6, fig.width=6}

predictions_random_effect <- fit |>
  ggpredict("scale_name", 
            type = "random", 
            condition = c(sample_size = 100, strategy = "match_item_if_dropped"), 
            ci.lvl = NA) |>
  as_tibble() |>
  rename(scale_name = x) |>
  mutate(scale_name = fct_rev(scale_name)) |>
  arrange(scale_name)

plot_re <- 
  ggplot(predictions_random_effect, aes(predicted, scale_name)) +
  geom_vline(xintercept = results_estimates |> filter(sample_size == 100 & strategy == "match_item_if_dropped") |> pull(marginalmean), 
             linetype = "dashed") +
  geom_point() +
  scale_x_continuous(
    #breaks = c(0, .25, .5, .75, 1),
    breaks = c(0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),
    #labels = c("0.00<br/>(Worse)", "0.25", "0.50", "0.75", "1.00<br/>(Better)"),
    #labels = c("0.00", "0.25", "0.50", "0.75", "1.00"),
    #labels = c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0"),
    limits = c(0,1)
  ) +
  labs(x = "Replication rate",
       y = "Scale") +
  mdthemes::md_theme_linedraw() +
  theme(panel.grid.minor.x = element_blank()) +
  guides(color = guide_legend(reverse = TRUE)) 

plot_re_with_density <- ggMarginal(plot_re, type = "density", fill = "lightgrey")

plot_re_with_density

ggsave(filename  = "../../../communication/plots/plot_replication_rate_bainbridge_et_al_by_scale.pdf",
       plot      = plot_re_with_density,
       device    = cairo_pdf, # for greek letters
       units     = "in",
       width     = 6,
       height    = 6,
       limitsize = TRUE)

```

### Table

```{r}

predictions_random_effect |>
  select(-group) |>
  round_df(2) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Session info

```{r}

sessionInfo()

```

