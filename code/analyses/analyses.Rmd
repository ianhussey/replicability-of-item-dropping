---
title: "Assessing the replicability of item dropping"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
```

```{r}

# dependencies
library(tidyverse)
#library(lavaan)
#library(semTools)
library(knitr)
library(kableExtra)
#library(moments)
#library(plotrix)
#library(lubridate)
library(psych)
library(furrr)
library(lme4)
library(sjPlot)
library(emmeans)
#library(ggstance)
library(janitor)
library(boot)

# create necessary directories
dir.create("../../data/results")

# functions
# rounds all numeric variables in a dataframe to the desired number of places. Non-numeric variables will be ignored.
round_df <- function(df, digits) {
  mutate_if(df, is.numeric, janitor::round_half_up, digits = 2)
}

# set up parallel processing
future::plan(multisession)

# set seed for reproducibility
set.seed(42)

```

# Data

```{r}

#data_first_timepoint <- read_rds("../../data/processed/data_first_timepoint.rds")
data_nested_single_timepoint <- read_rds("../../data/processed/data_nested_single_timepoint.rds")

```

# Full sample metrics

These are already established scales, many of which are well known, with a reasonable range of Cronbach's $\alpha$ values when calculated in a large sample. The subsequent assessment of the replicability of item-dropping recommendations is therefore likely generalizable. 

```{r}

cronbachs_alpha <- function(data){
  psych::alpha(data)$total["raw_alpha"]
}

results_overall <- data_nested_single_timepoint |> 
  mutate(results = furrr::future_map(data, 
                                     cronbachs_alpha, 
                                     .options = furrr_options(seed = TRUE))) |>
  unnest(results) |> 
  mutate(n = furrr::future_map(data, nrow, .options = furrr_options(seed = TRUE))) |>
  unnest(n)

results_overall |>
  select(scale, alpha = raw_alpha, n) |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

results_overall |>
  select(scale, alpha = raw_alpha, n) |>
  summarize(k_scales = n(),
            N_participants = sum(n),
            alpha_min = min(alpha),
            alpha_max = max(alpha),
            alpha_mean = mean(alpha),
            alpha_sd = sd(alpha)) |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# lm(raw_alpha ~ 1, weights = n, data = results_overall)
weighted_average <- janitor::round_half_up(weighted.mean(results_overall$raw_alpha), 2)

```

Weighted average $\alpha$ = `r weighted_average`.

# Create nested pairs of samples of data from each scale

```{r}

if(file.exists("../../data/intermediary/data_replications.rds")){
  
  data_replications <- read_rds("../../data/intermediary/data_replications.rds")
  
} else {
  
  generate_replications <- function(input_data, n_replications, subset_n_per_split){
    
    helper_subset_n <- function(dat, subset_n_per_split){sample_n(dat, size = subset_n_per_split*2)}
    
    res <- 
      tibble(replication = seq(from = 1, to = n_replications, by = 1)) |>
      mutate(nest(mutate(input_data, id = row_number()), data = everything()),
             data_subset = map(data, helper_subset_n, subset_n_per_split = subset_n_per_split),
             data_subset_a = map(data_subset, sample_frac, size = 0.5),
             data_subset_b = map2(data_subset, data_subset_a, anti_join, by = "id"),
             data_subset_a = map(data_subset_a, select, -id),
             data_subset_b = map(data_subset_b, select, -id)) |>
      select(-data, -data_subset)
    
    return(res)
  }
  
  data_nested_single_timepoint_25 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications, 
                                            n_replications = 1000, 
                                            subset_n_per_split = 25,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 25) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_50 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications, 
                                            n_replications = 1000, 
                                            subset_n_per_split = 50,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 50) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_100 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications,
                                            n_replications = 1000,
                                            subset_n_per_split = 100,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 100) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_250 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications,
                                            n_replications = 1000,
                                            subset_n_per_split = 250,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 250) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_500 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications,
                                            n_replications = 1000, 
                                            subset_n_per_split = 500,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 500) |>
    select(-data) |>
    unnest(replications)
  
  data_replications <- 
    bind_rows(data_nested_single_timepoint_25,
              data_nested_single_timepoint_50,
              data_nested_single_timepoint_100,
              data_nested_single_timepoint_250,
              data_nested_single_timepoint_500)
  
  write_rds(data_replications, "../../data/intermediary/data_replications.rds", compress = "gz")
  
}

```

# Calculate alpha and drop decisions for each sample

```{r}

if(file.exists("../../data/results/data_drop_decisions.rds")){
  
  data_drop_decisions <- read_rds("../../data/results/data_drop_decisions.rds")
  
} else {
  
  item_to_drop <- function(data){
    
    res <- psych::alpha(data)
    
    alpha_full_scale <- as.numeric(res$total["raw_alpha"])
    
    res$alpha.drop |>
      as_tibble(rownames = "item") |>
      filter(raw_alpha == max(raw_alpha)) |>
      select(item_to_drop = item, alpha_if_dropped = raw_alpha) |>
      mutate(alpha_full_scale = alpha_full_scale,
             item_to_drop_or_none = ifelse(alpha_full_scale >= alpha_if_dropped, "none", item_to_drop))
  }
  
  data_drop_decisions <- data_replications |>
    # subset A
    mutate(alpha_a = furrr::future_map(data_subset_a, item_to_drop)) |>
    unnest(alpha_a) |>
    rename(item_to_drop_a         = item_to_drop,
           item_to_drop_or_none_a = item_to_drop_or_none,
           alpha_if_dropped_a     = alpha_if_dropped,
           alpha_full_scale_a     = alpha_full_scale) |>
    # subset B
    mutate(alpha_b = furrr::future_map(data_subset_b, item_to_drop)) |>
    unnest(alpha_b) |>
    rename(item_to_drop_b         = item_to_drop,
           item_to_drop_or_none_b = item_to_drop_or_none,
           alpha_if_dropped_b     = alpha_if_dropped,
           alpha_full_scale_b     = alpha_full_scale) |>
    # compare
    mutate(match_item_if_dropped = item_to_drop_a == item_to_drop_b,
           match_item_if_dropped_or_none = item_to_drop_or_none_a == item_to_drop_or_none_b)
  
  write_rds(data_drop_decisions, "../../data/results/data_drop_decisions.rds")
  
}

  
alpha_if_given_item_dropped <- function(data, item_to_drop){
  
  res <- data |>
    select(-{{item_to_drop}}) |>
    psych::alpha()
  
  alpha <- as.numeric(res$total["raw_alpha"])
  
  return(alpha)
}

data_drop_decisions2 <- data_drop_decisions |>
  mutate(alpha_if_a_recommendation_dropped_b = furrr::future_map2(data_subset_b, 
                                                                  item_to_drop_a,
                                                                  alpha_if_given_item_dropped,
                                                                  .options = furrr_options(seed = TRUE)))

write_rds(data_drop_decisions2, "../../data/results/data_drop_decisions2.rds")


```

# Summarize drop decisions

```{r}

data_drop_decisions_summary <- data_drop_decisions %>%
  group_by(scale) |>
  summarize(proportion_match_item_if_dropped = mean(match_item_if_dropped, na.rm = TRUE),
            variance_match_item_if_dropped = possibly(var, otherwise = NA_real_)(match_item_if_dropped),
            proportion_match_item_if_dropped_or_none = mean(match_item_if_dropped_or_none, na.rm = TRUE),
            variance_match_item_if_dropped_or_none = possibly(var, otherwise = NA_real_)(match_item_if_dropped_or_none),
            #subset_n_per_split = subset_n_per_split,
            n_replications     = max(replication))

```

# Results

```{r}

data_reshaped <- results_nested |>
  unnest(results_25, results_50, results_100, results_200, results_300) |>
  select(scale,
         proportion_25  = proportion,
         proportion_50  = proportion1,
         proportion_100 = proportion2,
         proportion_200 = proportion3,
         proportion_300 = proportion4,
         variance_25    = variance,
         variance_50    = variance1,
         variance_100   = variance2,
         variance_200   = variance3,
         variance_300   = variance4) |>
  pivot_longer(names_to = c("type", "n_participants"), 
               names_pattern = "(.*)_(.*)",
               values_to = c("value"),
               cols = c("proportion_25", 
                        "proportion_50", 
                        "proportion_100", 
                        "proportion_200", 
                        "proportion_300",
                        "variance_25",
                        "variance_50",
                        "variance_100",
                        "variance_200",
                        "variance_300")) |>
  pivot_wider(names_from = "type", 
              values_from = "value") |>
  # offset problematic values, logit transform
  mutate(proportion = case_when(proportion == 0 ~ 0.001,
                                proportion == 1 ~ 0.999,
                                TRUE ~ proportion),
         proportion_logit = boot::logit(proportion),
         variance = ifelse(variance == 0, 0.001, variance)) |>
  # order factors
  mutate(n_participants = fct_reorder(n_participants, as.numeric(n_participants)))

data_reshaped |>
  select(scale, n_participants, proportion) |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Meta

```{r}

# fit model
fit <-
  lmer(proportion_logit ~ 1 + n_participants + (1 | scale),
       weights = 1/variance,
       data = data_reshaped)

# extract re Tau
results_re_tau <- fit |>
  merTools::REsdExtract() |>
  as_tibble(rownames = "scale") |>
  rename(tau = value)

# extract marginal means
results <-
  summary(emmeans(fit, ~ n_participants)) |>
  dplyr::select(n_participants, estimate = emmean, se = SE, 
                ci_lower = lower.CL, ci_upper = upper.CL) |>
  mutate(pi_lower = estimate - (1.96 * sqrt(se^2 + results_re_tau$tau^2)),
         pi_upper = estimate + (1.96 * sqrt(se^2 + results_re_tau$tau^2))) |>
  select(-se) |>
  mutate_if(is.numeric, boot::inv.logit)

# plot
p_meta <-
  ggplot(results, aes(estimate, fct_rev(n_participants))) +
  geom_linerange(aes(xmin = pi_lower, xmax = pi_upper), size = 0.5, linetype = "dotted") +
  geom_linerange(aes(xmin = ci_lower, xmax = ci_upper)) +
  geom_point(size = 2.5) +
  mdthemes::md_theme_linedraw() +
  scale_x_continuous(breaks = c(0, .25, .5, .75, 1), 
                     labels = c("0.00<br/>(Worse)", "0.25", "0.50", "0.75", "1.00<br/>(Better)")) +
  labs(x = "Proportion of cases where<br/>item-dropping recommendation replicated",
       y = "N participants in each sample") +
  #theme(legend.position = "none") +
  xlim(0, 1)

p_meta

results |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# # tests
# data_emms <- emmeans(fit, list(pairwise ~ n_participants), adjust = "holm")
# 
# summary(data_emms)$`pairwise differences of n_participants` |>
#   as.data.frame() |>
#   select(comparison = 1, p.value) |>
#   mutate(p.value = ifelse(p.value < .001, "< .001", round_half_up(p.value, 3))) |>
#   kable() |>
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Session info

```{r}

sessionInfo()

```

# old Analysis

```{r}

if(file.exists("../../data/results/results_nested.rds")){
  
  results_nested <- read_rds("../../data/results/results_nested.rds")
  
} else {
  
  # assess which item, if dropped, would result in the highest alpha in the remaining items
  item_to_delete_to_max_alpha <- function(data){
    
    res <- psych::alpha(data)
    
    alpha_full_scale <- as.numeric(res$total["raw_alpha"])
    
    res$alpha.drop |>
      as_tibble(rownames = "item") |>
      filter(raw_alpha == max(raw_alpha)) |>
      select(item_to_drop = item, alpha_if_droppped = raw_alpha) |>
      mutate(alpha_full_scale = alpha_full_scale)
  }
  
  # apply item_to_delete_to_max_alpha to each half of (a subset of) a data set
  # optionally first sample just a subset of participants
  # then split in half and calculate the item to be dropped for each half
  # returns comparison of whether the item drop recommendation was the same in both halves
  alpha_by_split <- function(data = data, subset_n_per_split = NULL){
    
    dat <- data |>
      rownames_to_column(var = "id")
    
    if(is.numeric(subset_n_per_split)){
      dat <- dat |>
        sample_n(size = subset_n_per_split*2)
    } 
    
    data_split_1 <- dat |>
      sample_frac(size = 0.5)
    
    data_split_2 <- anti_join(dat, data_split_1, by = "id")
    
    item_drop_split_1 <- data_split_1 |>
      select(-id) |>
      item_to_delete_to_max_alpha() |>
      rename(split_1_item_to_drop      = item_to_drop,
             split_1_alpha_if_droppped = alpha_if_droppped,
             split_1_alpha_full_scale  = alpha_full_scale)
    
    item_drop_split_2 <- data_split_2 |>
      select(-id) |>
      item_to_delete_to_max_alpha() |>
      rename(split_2_item_to_drop      = item_to_drop,
             split_2_alpha_if_droppped = alpha_if_droppped,
             split_2_alpha_full_scale  = alpha_full_scale)
    
    res <- bind_cols(item_drop_split_1, item_drop_split_2) |>
      mutate(match = split_1_item_to_drop == split_2_item_to_drop,
             alpha_diff_full_scale = split_1_alpha_full_scale - split_2_alpha_full_scale)
    
    return(res)
  }
  
  # apply alpha_by_split() to nested data
  # apply it n_replications of times to each cell (i.e., nest)
  # summarize the proportion of replications in which the recommendation agreed between samples
  alpha_by_split_replicate <- function(data, subset_n_per_split, n_replications, ...){
    
    replications <-
      replicate(n = n_replications,
                alpha_by_split(data = data, subset_n_per_split = subset_n_per_split)$match)
    
    tibble(proportion         = mean(replications, na.rm = TRUE),
           variance           = possibly(var, otherwise = NA_real_)(replications),
           subset_n_per_split = subset_n_per_split,
           n_replications     = n_replications)
  }
  
  n_replications <- 1000
  
  # run analyses using the data
  results_nested <- data_nested_single_timepoint |>
    mutate(results_25 = furrr::future_map(data,
                                          alpha_by_split_replicate,
                                          subset_n_per_split = 25,
                                          n_replications = n_replications,
                                          .options = furrr_options(seed = TRUE)),
           results_50 = furrr::future_map(data,
                                          alpha_by_split_replicate,
                                          subset_n_per_split = 50,
                                          n_replications = n_replications,
                                          .options = furrr_options(seed = TRUE)),
           results_100 = furrr::future_map(data,
                                           alpha_by_split_replicate,
                                           subset_n_per_split = 100,
                                           n_replications = n_replications,
                                           .options = furrr_options(seed = TRUE)),
           results_200 = furrr::future_map(data,
                                           alpha_by_split_replicate,
                                           subset_n_per_split = 200,
                                           n_replications = n_replications,
                                           .options = furrr_options(seed = TRUE)),
           results_300 = furrr::future_map(data,
                                           alpha_by_split_replicate,
                                           subset_n_per_split = 300,
                                           n_replications = n_replications,
                                           .options = furrr_options(seed = TRUE)))
  
  results_nested <- data_nested_single_timepoint |>
    mutate(results_500 = furrr::future_map(data,
                                           alpha_by_split_replicate,
                                           subset_n_per_split = 500,
                                           n_replications = n_replications,
                                           .options = furrr_options(seed = TRUE)))
  
  # save to disk
  write_rds(results_nested, "../../data/results/results_nested.rds")
}

```
