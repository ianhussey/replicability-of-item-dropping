---
title: "Assessing the replicability of item dropping"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
```

```{r}

# dependencies
library(tidyverse)
#library(lavaan)
#library(semTools)
library(knitr)
library(kableExtra)
#library(moments)
#library(plotrix)
#library(lubridate)
library(psych)
library(furrr)
library(lme4)
library(sjPlot)
library(emmeans)
#library(ggstance)
library(janitor)
library(boot)

# create necessary directories
dir.create("../../data/results")

# functions
# rounds all numeric variables in a dataframe to the desired number of places. Non-numeric variables will be ignored.
round_df <- function(df, digits) {
  mutate_if(df, is.numeric, janitor::round_half_up, digits = 2)
}

# set up parallel processing
future::plan(multisession)

# set seed for reproducibility
set.seed(42)

```

# Data

```{r}

#data_first_timepoint <- read_rds("../../data/processed/data_first_timepoint.rds")
data_nested_single_timepoint <- read_rds("../../data/processed/data_nested_single_timepoint.rds")

```

# Full sample metrics

These are already established scales, many of which are well known, with a reasonable range of Cronbach's $\alpha$ values when calculated in a large sample. The subsequent assessment of the replicability of item-dropping recommendations is therefore likely generalizable. 

```{r}

cronbachs_alpha <- function(data){
  psych::alpha(data)$total["raw_alpha"]
}

results_overall <- data_nested_single_timepoint |> 
  mutate(results = furrr::future_map(data, 
                                     cronbachs_alpha, 
                                     .options = furrr_options(seed = TRUE))) |>
  unnest(results) |> 
  mutate(n = furrr::future_map(data, nrow, .options = furrr_options(seed = TRUE))) |>
  unnest(n)

results_overall |>
  select(scale, alpha = raw_alpha, n) |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

results_overall |>
  select(scale, alpha = raw_alpha, n) |>
  summarize(k_scales = n(),
            N_participants = sum(n),
            alpha_min = min(alpha),
            alpha_max = max(alpha),
            alpha_mean = mean(alpha),
            alpha_sd = sd(alpha)) |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# lm(raw_alpha ~ 1, weights = n, data = results_overall)
weighted_average <- janitor::round_half_up(weighted.mean(results_overall$raw_alpha), 2)

```

Weighted average $\alpha$ = `r weighted_average`.

# Create nested pairs of samples of data from each scale

```{r}

if(file.exists("../../data/intermediary/data_replications.rds")){
  
  data_replications <- read_rds("../../data/intermediary/data_replications.rds")
  
} else {
  
  generate_replications <- function(input_data, n_replications, subset_n_per_split){
    
    helper_subset_n <- function(dat, subset_n_per_split){sample_n(dat, size = subset_n_per_split*2)}
    
    res <- 
      tibble(replication = seq(from = 1, to = n_replications, by = 1)) |>
      mutate(nest(mutate(input_data, id = row_number()), data = everything()),
             data_subset = map(data, helper_subset_n, subset_n_per_split = subset_n_per_split),
             data_subset_a = map(data_subset, sample_frac, size = 0.5),
             data_subset_b = map2(data_subset, data_subset_a, anti_join, by = "id"),
             data_subset_a = map(data_subset_a, select, -id),
             data_subset_b = map(data_subset_b, select, -id)) |>
      select(-data, -data_subset)
    
    return(res)
  }
  
  data_nested_single_timepoint_25 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications, 
                                            n_replications = 1000, 
                                            subset_n_per_split = 25,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 25) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_50 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications, 
                                            n_replications = 1000, 
                                            subset_n_per_split = 50,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 50) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_100 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications,
                                            n_replications = 1000,
                                            subset_n_per_split = 100,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 100) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_250 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications,
                                            n_replications = 1000,
                                            subset_n_per_split = 250,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 250) |>
    select(-data) |>
    unnest(replications)
  
  data_nested_single_timepoint_500 <- data_nested_single_timepoint |>
    mutate(replications = furrr::future_map(data, 
                                            generate_replications,
                                            n_replications = 1000, 
                                            subset_n_per_split = 500,
                                            .options = furrr_options(seed = TRUE)),
           subset_n_per_split = 500) |>
    select(-data) |>
    unnest(replications)
  
  data_replications <- 
    bind_rows(data_nested_single_timepoint_25,
              data_nested_single_timepoint_50,
              data_nested_single_timepoint_100,
              data_nested_single_timepoint_250,
              data_nested_single_timepoint_500)
  
  write_rds(data_replications, "../../data/intermediary/data_replications.rds", compress = "gz")
  
}

```

# Calculate alpha and drop decisions for each sample

```{r}

if(file.exists("../../data/results/data_drop_decisions.rds")){
  
  data_drop_decisions <- read_rds("../../data/results/data_drop_decisions.rds")
  
} else {
  
  item_to_drop <- function(data){
    
    res <- psych::alpha(data)
    
    alpha_full_scale <- as.numeric(res$total["raw_alpha"])
    
    res$alpha.drop |>
      as_tibble(rownames = "item") |>
      filter(raw_alpha == max(raw_alpha)) |>
      select(item_to_drop = item, alpha_if_dropped = raw_alpha) |>
      mutate(alpha_full_scale = alpha_full_scale,
             item_to_drop_or_none = ifelse(alpha_full_scale >= alpha_if_dropped, "none", item_to_drop))
  }
  
  data_drop_decisions <- data_replications |>
    # subset A
    mutate(alpha_a = furrr::future_map(data_subset_a, item_to_drop)) |>
    unnest(alpha_a) |>
    rename(item_to_drop_a         = item_to_drop,
           item_to_drop_or_none_a = item_to_drop_or_none,
           alpha_if_dropped_a     = alpha_if_dropped,
           alpha_full_scale_a     = alpha_full_scale) |>
    # subset B
    mutate(alpha_b = furrr::future_map(data_subset_b, item_to_drop)) |>
    unnest(alpha_b) |>
    rename(item_to_drop_b         = item_to_drop,
           item_to_drop_or_none_b = item_to_drop_or_none,
           alpha_if_dropped_b     = alpha_if_dropped,
           alpha_full_scale_b     = alpha_full_scale) |>
    # compare
    mutate(match_item_if_dropped = item_to_drop_a == item_to_drop_b,
           match_item_if_dropped_or_none = item_to_drop_or_none_a == item_to_drop_or_none_b)
  
  write_rds(data_drop_decisions, "../../data/results/data_drop_decisions.rds", compress = "gz")
  
  data_drop_decisions_no_data <- data_drop_decisions |>
    select(-data_subset_a, -data_subset_b)
  
  write_rds(data_drop_decisions_no_data, "../../data/results/data_drop_decisions_no_data.rds", compress = "gz")
  
}

  
alpha_if_given_item_dropped <- function(data, item_to_drop){
  
  res <- data |>
    select(-{{item_to_drop}}) |>
    psych::alpha()
  
  alpha <- as.numeric(res$total["raw_alpha"])
  
  return(alpha)
}

data_drop_decisions2 <- data_drop_decisions |>
  mutate(alpha_if_a_recommendation_dropped_b = furrr::future_map2(data_subset_b, 
                                                                  item_to_drop_a,
                                                                  alpha_if_given_item_dropped,
                                                                  .options = furrr_options(seed = TRUE)))

write_rds(data_drop_decisions2, "../../data/results/data_drop_decisions2.rds")


```

# Summarize drop decisions

```{r}

data_drop_decisions_summary <- data_drop_decisions_no_data %>%
  group_by(scale, subset_n_per_split) |>
  summarize(proportion_match_item_if_dropped = mean(match_item_if_dropped, na.rm = TRUE),
            variance_match_item_if_dropped = possibly(var, otherwise = NA_real_)(match_item_if_dropped),
            proportion_match_item_if_dropped_or_none = mean(match_item_if_dropped_or_none, na.rm = TRUE),
            variance_match_item_if_dropped_or_none = possibly(var, otherwise = NA_real_)(match_item_if_dropped_or_none),
            #subset_n_per_split = subset_n_per_split,
            n_replications = max(replication))

```

# Results

```{r}

data_reshaped <- data_drop_decisions_summary |>
  # offset problematic values, logit transform
  mutate(proportion_match_item_if_dropped = case_when(proportion_match_item_if_dropped == 0 ~ 0.001,
                                                      proportion_match_item_if_dropped == 1 ~ 0.999,
                                                      TRUE ~ proportion_match_item_if_dropped),
         proportion_match_item_if_dropped_logit = boot::logit(proportion_match_item_if_dropped),
         variance_match_item_if_dropped = ifelse(variance_match_item_if_dropped == 0, 0.001, variance_match_item_if_dropped),
         proportion_match_item_if_dropped_or_none = case_when(proportion_match_item_if_dropped_or_none == 0 ~ 0.001,
                                                              proportion_match_item_if_dropped_or_none == 1 ~ 0.999,
                                                              TRUE ~ proportion_match_item_if_dropped_or_none),
         proportion_match_item_if_dropped_or_none_logit = boot::logit(proportion_match_item_if_dropped_or_none),
         variance_match_item_if_dropped_or_none = ifelse(variance_match_item_if_dropped_or_none == 0, 0.001, variance_match_item_if_dropped_or_none)) |>
  # order factors
  mutate(n_participants = fct_reorder(as.factor(subset_n_per_split), as.numeric(subset_n_per_split)))

data_reshaped |>
  select(scale, n_participants, 
         proportion_match_item_if_dropped, proportion_match_item_if_dropped_or_none) |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Meta

```{r}

# fit model
fit <-
  lmer(proportion_match_item_if_dropped_logit ~ 1 + n_participants + (1 | scale),
       weights = 1/variance_match_item_if_dropped,
       data = data_reshaped)

# extract re Tau
results_re_tau <- fit |>
  merTools::REsdExtract() |>
  as_tibble(rownames = "scale") |>
  rename(tau = value)

# extract marginal means
results <-
  summary(emmeans(fit, ~ n_participants)) |>
  dplyr::select(n_participants, estimate = emmean, se = SE, 
                ci_lower = lower.CL, ci_upper = upper.CL) |>
  mutate(pi_lower = estimate - (1.96 * sqrt(se^2 + results_re_tau$tau^2)),
         pi_upper = estimate + (1.96 * sqrt(se^2 + results_re_tau$tau^2))) |>
  select(-se) |>
  mutate_if(is.numeric, boot::inv.logit)

# plot
p_meta <-
  ggplot(results, aes(estimate, fct_rev(n_participants))) +
  geom_linerange(aes(xmin = pi_lower, xmax = pi_upper), size = 0.5, linetype = "dotted") +
  geom_linerange(aes(xmin = ci_lower, xmax = ci_upper)) +
  geom_point(size = 2.5) +
  mdthemes::md_theme_linedraw() +
  scale_x_continuous(breaks = c(0, .25, .5, .75, 1), 
                     labels = c("0.00<br/>(Worse)", "0.25", "0.50", "0.75", "1.00<br/>(Better)")) +
  labs(x = "Proportion of cases where<br/>item-dropping recommendation replicated",
       y = "N participants in each sample") +
  #theme(legend.position = "none") +
  xlim(0, 1)

p_meta

results |>
  round_df(2) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# # tests
# data_emms <- emmeans(fit, list(pairwise ~ n_participants), adjust = "holm")
# 
# summary(data_emms)$`pairwise differences of n_participants` |>
#   as.data.frame() |>
#   select(comparison = 1, p.value) |>
#   mutate(p.value = ifelse(p.value < .001, "< .001", round_half_up(p.value, 3))) |>
#   kable() |>
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Session info

```{r}

sessionInfo()

```

