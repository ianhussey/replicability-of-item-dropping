---
title: "Assessing the replicability of item-dropping decisions based on Cronbach's-alpha-if-item-removed"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# TODO

- p_freq seems to be missing scales and contain NAs, figure out why
- change fractional logit models to sth else?

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
```

```{r}

# dependencies
library(tidyverse)
library(knitr)
library(kableExtra)
library(psych)
library(lme4)
library(sjPlot)
library(scales)
library(ggstance)
library(marginaleffects)
library(janitor)
library(boot)
library(Cairo)

# functions
# rounds all numeric variables in a dataframe to the desired number of places. Non-numeric variables will be ignored.
round_df <- function(df, digits) {
  mutate_if(df, is.numeric, janitor::round_half_up, digits = 2)
}

dir.create("models")

```

# Data

```{r}

data_scale_names <- read_csv("../../../data/processed/bainbridge et al/scale abbreviations and full names.csv") |>
  mutate(scale = ifelse(is.na(subscale), scale, paste(scale, subscale, sep = "_"))) |>
  select(scale, scale_name)

data_nested <- read_rds("../../../data/processed/bainbridge et al/data_nested.rds") |>
  left_join(data_scale_names, by = "scale") |>
  select(-scale) |>
  rename(scale = scale_name)

data_drop_decisions_no_data <- read_rds("../../../data/processed/bainbridge et al/data_drop_decisions_no_data.rds") |>
  rename(sample_size = subset_n_per_split) |>
  mutate(sample_size                   = as.factor(sample_size),
         match_item_if_dropped         = as.factor(match_item_if_dropped),
         match_item_if_dropped_or_none = as.factor(match_item_if_dropped_or_none)) |>
  left_join(data_scale_names, by = "scale") |>
  select(-scale) |>
  rename(scale = scale_name)

```

# Full sample metrics

## Inter-item correlations

### Summary 

```{r fig.height=8, fig.width=8}

lower_half_of_cor_matrix <- function(data){
  correlations <- cor(as.data.frame(data))
  
  correlations[upper.tri(correlations, diag = TRUE)] <- NA
  
  return(correlations)
}

inter_item_correlations <- data_nested |> 
  mutate(results = map(data, lower_half_of_cor_matrix)) |>
  select(-data)


n_items <- data_nested |> 
  mutate(n_items = map_dbl(data, ncol)) |>
  select(-data)

ggplot(n_items, aes(x = n_items)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "darkgreen", alpha = 0.75) +
  #stat_function(fun = dnorm, args = list(mean = m, sd = SD), color = "grey30", size = 1) +
  ylab("Density") +
  xlab("N items per scale") +
  scale_x_continuous(breaks = scales::breaks_pretty(n = 10)) +
  theme_linedraw()

summary_inter_item_correlations <- inter_item_correlations |>
  mutate(mean_r = map_dbl(results, ~ mean(.x, na.rm = TRUE)),
         sd_r = map_dbl(results, ~ sd(.x, na.rm = TRUE)),
         min_r = map_dbl(results, ~ min(.x, na.rm = TRUE)),
         max_r = map_dbl(results, ~ max(.x, na.rm = TRUE))) |>
  select(-results) |>
  round_df() |>
  left_join(n_items, by = "scale") |>
  relocate(n_items, .before = mean_r) 
  #mutate(scale = fct_reorder(scale, mean_r))

summary_inter_item_correlations |>
  kable() |>
  kable_classic(full_width = FALSE)

ggplot(summary_inter_item_correlations, aes(mean_r, scale)) +
  geom_linerangeh(aes(xmin = mean_r - sd_r, xmax = mean_r + sd_r)) + # , size = 1
  #geom_linerangeh(aes(xmin = min_r, xmax = max_r)) +
  # geom_point(aes(x = min_r), shape = 3) +
  # geom_point(aes(x = max_r), shape = 3) +
  geom_point(aes(x = min_r), shape = "|") +
  geom_point(aes(x = max_r), shape = "|") +
  scale_x_continuous(breaks = breaks_pretty(n = 11)) +
  geom_point() +
  ylab("") +
  xlab("Inter-item correlation\n(mean Â± SD, vertical bars for min and max)") +
  theme_linedraw()

```

### Distributions

```{r}

# convert each matrix in 'results' to a long format data frame and combine them into a single data frame
inter_item_correlations_long <- inter_item_correlations %>%
  mutate(long_data = map(results, ~ .x %>%
                           as_tibble(rownames = "var1") %>%
                           pivot_longer(cols = -var1, names_to = "var2", values_to = "r") %>%
                           drop_na(r))) %>%
  select(-results) %>%
  unnest(long_data)


m <- mean(inter_item_correlations_long$r, na.rm = TRUE)
SD <- sd(inter_item_correlations_long$r, na.rm = TRUE)

ggplot(inter_item_correlations_long, aes(x = r)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, boundary = 0, fill = "darkgreen", alpha = 0.75) +
  stat_function(fun = dnorm, args = list(mean = m, sd = SD), color = "grey30", size = 1) +
  ylab("Density") +
  xlab("Inter-item correlation") +
  scale_x_continuous(breaks = scales::breaks_pretty(n = 5)) +
  theme_linedraw()

```

For the full sample of scales, the mean inter-item correlation was `r janitor::round_half_up(m, 2)`, SD = `r janitor::round_half_up(SD, 2)`.

```{r fig.height=16, fig.width=9}

ggplot(inter_item_correlations_long, aes(r)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_histogram(binwidth = 0.1, boundary = 0, fill = "darkgreen", alpha = 0.75) +
  #geom_vline(xintercept = m, linetype = "dashed") +
  ylab("") +
  xlab("Inter-item correlation") +
  scale_x_continuous(breaks = scales::breaks_pretty(n = 5)) +
  scale_y_continuous(breaks = scales::breaks_pretty()) +
  theme_linedraw() +
  facet_wrap(~ scale, scales = "free_y", ncol = 4) +
  theme(panel.grid.minor = element_blank())

```

## Full scale Cronbach's $\alpha$

These are already established scales, many of which are well known, with a reasonable range of Cronbach's $\alpha$ values when calculated in a large sample. The subsequent assessment of the replicability of item-dropping recommendations is therefore likely generalizable. 

```{r}

cronbachs_alpha <- function(data){
  psych::alpha(data)$total["raw_alpha"]
}

results_overall <- data_nested |> 
  mutate(results = map(data, cronbachs_alpha)) |>
  unnest(results) |> 
  mutate(n = map(data, nrow)) |>
  unnest(n) |>
  left_join(data_scale_names, by = "scale")

results_overall |>
  select(scale, alpha = raw_alpha, n) |>
  round_df(2) |>
  kable() |>
  kable_classic(full_width = FALSE)

results_overall |>
  select(scale, alpha = raw_alpha, n) |>
  summarize(k_scales       = n(),
            alpha_min      = min(alpha),
            alpha_max      = max(alpha)) |>
  round_df(2) |>
  kable() |>
  kable_classic(full_width = FALSE)

weighted_mean <- janitor::round_half_up(weighted.mean(results_overall$raw_alpha, results_overall$n), 2)

weighted_sd <- janitor::round_half_up(sqrt(Hmisc::wtd.var(results_overall$raw_alpha, results_overall$n)), 2)

```

Weighted mean $\alpha$ = `r weighted_mean`, weighted SD =  `r weighted_sd`.

# Frequency of item drop recommendations

```{r fig.height=20, fig.width=14}

data_n_items <- data_nested |>
  mutate(n_items_in_scale = map_dbl(data, ncol)) |>
  select(-data)

data_drop_decisions_no_data |>
  distinct(scale, item_to_drop_a) |>
  count(scale) |>
  rename(n_distinct_items_with_drop_recommendations = n) |>
  left_join(data_n_items, by = "scale") |>
  kable() |>
  kable_classic(full_width = FALSE)

# data_drop_decisions_no_data |>
#   distinct(scale)

data_drop_decision_frequencies <- data_drop_decisions_no_data |>
  count(scale, item_to_drop_a) |>
  arrange(scale, desc(n)) |>
  group_by(scale) |>
  mutate(item_rank = paste("ranked_item_", row_number(), sep = ""),
         item_rank = fct_reorder(item_rank, n, .desc = TRUE)) |>
  ungroup() |>
  select(-item_to_drop_a) |>
  filter(item_rank %in% c("ranked_item_1",
                          "ranked_item_2",
                          "ranked_item_3",
                          "ranked_item_4",
                          "ranked_item_5"))

p_freq <- 
  ggplot(data_drop_decision_frequencies, aes(item_rank, n)) +
  geom_bar(stat = "identity") +
  scale_x_discrete(labels = seq(from = 1, to = max(data_n_items$n_items_in_scale), by = 1)) +
  facet_wrap(~ scale, ncol = 4) +
  xlab("Item ranked by number of drop recommendations") +
  ylab("Number of drop recommendations") +
  theme_light()

p_freq

ggsave(filename  = "../../../communication/plots/plot_drop_recommendation_frequencies_bainbridge_et_al.pdf",
       plot      = p_freq,
       device    = "pdf",
       units     = "in",
       width     = 14,
       height    = 12,
       limitsize = TRUE)

```

# Replication rate of item-dropping recommendations

## Observed rates

Strategy 1: Drop one item based on max Cronbach's-$\alpha$-if-item-removed. I.e, item dropping was mandatory, even if the alpha in the retained items was lower than the full-scale $\alpha$. Recommendation in each sample could be item 1...item N.

Strategy 2: Drop an item based on max Cronbach's-$\alpha$-if-item-removed if it is higher than the full-scale $\alpha$. I.e, item dropping was not mandatory if it didn't improve $\alpha$. Recommendation in each sample could be item 1...item N or no item dropped.

```{r fig.height=8, fig.width=10}

dat <- data_drop_decisions_no_data |>
  select(scale, replication, sample_size, match_item_if_dropped, match_item_if_dropped_or_none) |>
  pivot_longer(names_to = "strategy",
               values_to = "replicated",
               cols = c(match_item_if_dropped, match_item_if_dropped_or_none))

dat_replication_rate <- dat |>
  group_by(scale, strategy, sample_size) |>
  summarize(replication_rate = mean(as.logical(replicated))) |>
  mutate(sample_size = paste0("N = ", sample_size),
         sample_size = fct_relevel(sample_size, "N = 25", "N = 50", "N = 100", "N = 250", "N = 500")) 

p1 <- 
  ggplot(dat_replication_rate, aes(replication_rate, fct_rev(scale), color = strategy)) +
  #geom_linerangeh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(position = position_dodge(width = 0.5), shape = "square") +
  ylab("") +
  facet_wrap(~ sample_size, nrow = 1) +
  scale_color_viridis_d(option = "mako", begin = 0.3, end = 0.7,
                        name = "Item dropping strategy",
                        labels = c("Mandatory dropping", "Dropping only if it improves alpha"),
                        guide = guide_legend(reverse = TRUE)) +
  scale_x_continuous(breaks = breaks_width(.20), 
                     limits = c(0,1)) +
  theme_linedraw() +
  theme(legend.position = "bottom") +
  xlab("Replication rate")

p1

```

```{r fig.height=8, fig.width=8}

# dat_replication_rate_wide <- dat_replication_rate |>
#   pivot_wider(names_from = strategy,
#               values_from = replication_rate) |>
#   mutate(diff = match_item_if_dropped - match_item_if_dropped_or_none) |>
#   select(-match_item_if_dropped, -match_item_if_dropped_or_none)
# 
# dat_replication_rate_combined <- 
#   left_join(dat_replication_rate, 
#             dat_replication_rate_wide,
#             by = join_by(scale, sample_size)) |>
#   mutate(scale = fct_reorder(scale, diff))

p2 <-
  dat_replication_rate |>
  filter(sample_size == "N = 100") |>
  ggplot(aes(replication_rate, fct_rev(scale), color = strategy)) +
  #geom_linerangeh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(position = position_dodge(width = 0.5), shape = "square") +
  ylab("") +
  scale_color_viridis_d(option = "mako", begin = 0.3, end = 0.7,
                        name = "Item dropping strategy",
                        labels = c("Mandatory dropping", "Dropping only if it improves alpha"),
                        guide = guide_legend(reverse = TRUE)) +
  scale_x_continuous(breaks = breaks_width(.20),
                     limits = c(0,1)) +
  theme_linedraw() +
  theme(legend.position = "bottom") +
  xlab("Replication rate")

p2

```

## Model

Using multilevel logistic models

```{r}

if(file.exists("models/fit.rds")){
  
  fit <- read_rds("models/fit.rds")
  
} else {
  
  fit <- 
    glmer(replicated ~ 1 + sample_size * strategy + (1 | scale),
          family = binomial(link = "logit"),
          data = dat)
  
  write_rds(fit, file = "models/fit.rds")

}

# results table
tab_model(fit)

# marginal means
results_estimates <- fit |>
  marginalmeans(variables = c("sample_size", "strategy"), cross = TRUE) |>
  select(sample_size, strategy, marginalmean, ci_lower = conf.low, ci_upper = conf.high)

```

### Plot

```{r fig.height=4, fig.width=6}

plot_fe <- 
  ggplot(results_estimates, aes(marginalmean, sample_size, color = strategy)) +
  geom_linerangeh(aes(xmin = ci_lower, xmax = ci_upper), position = position_dodge(width = 0.5)) +
  geom_point(size = 2.5, shape = 15, position = position_dodge(width = 0.5)) +
  scale_color_viridis_d(begin = 0.3, end = 0.7,
                        labels = c("Max \u03B1 if item removed",
                                   "Max \u03B1 if item removed<br/>if it improves \u03B1 compared to full scale"),
                        name = "Item dropping strategy") +
  scale_x_continuous(
    #breaks = c(0, .25, .5, .75, 1), 
    breaks = c(0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0), 
    #labels = c("0.00<br/>(Worse)", "0.25", "0.50", "0.75", "1.00<br/>(Better)"),
    #labels = c("0.00", "0.25", "0.50", "0.75", "1.00"),
    #labels = c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0"),
    limits = c(0,1)
  ) +
  labs(x = "Replication rate",
       y = "Sample size") +
  mdthemes::md_theme_linedraw() +
  theme(legend.position = c(0.73, 0.18), # "right",
        legend.key.height = unit(0.05, 'npc'),
        panel.grid.minor.x = element_blank(),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  guides(color = guide_legend(reverse = TRUE))

plot_fe

ggsave(filename  = "../../../communication/plots/plot_replication_rate_bainbridge_et_al.pdf",
       plot      = plot_fe,
       device    = cairo_pdf, # for greek letters
       units     = "in",
       width     = 6,
       height    = 4,
       limitsize = TRUE)

```

### Table

```{r}

results_estimates |>
  round_df(2) |>
  arrange(sample_size, strategy) |>
  select(strategy, sample_size, marginalmean, ci_lower, ci_upper) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Session info

```{r}

sessionInfo()

```

