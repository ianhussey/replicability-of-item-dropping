---
title: "Alpha hacking simulation"
author: "Ruben Arslan"
date: "2023-11-07"
output: 
  html_document:
    toc: true
    toc_float: true
---


```{r warning=F,message=F}
knitr::opts_chunk$set(echo = TRUE, error = T, warning = F, message = F)

# Libraries and Settings

# Libs ---------------------------
library(tidyverse)
library(arrow)
library(glue)
library(psych)
library(lavaan)
library(ggplot2)
library(plotly)
library(gridExtra)
library(semTools)
library(semPlot)

model_name = "ItemSimilarityTraining-20240502-trial12"
#model_name = "item-similarity-20231018-122504"
pretrained_model_name = "all-mpnet-base-v2"

data_path = glue("../../../data/processed/bainbridge et al - new scale creation/")
pretrained_data_path = glue("./")

set.seed(42)

number_of_items <- 246
number_of_scales <- 76
number_of_scales_with_more_than_3_items <- 55
combinations_items <- choose(number_of_items, 2)
combinations_scales <- choose(number_of_scales, 2)
combinations_scales <- 2811 # after eliminating scale-subscale pairs
planned_N <- 400
```


## Precision simulation for synthetic inter-item correlations
```{r}
holdout <- arrow::read_feather(file = file.path(data_path, glue("ignore.{model_name}.raw.osf-bainbridge-2021-s2-0.item_correlations.feather")))
bainbridge <- arrow::read_feather(file = file.path(data_path, glue("ignore.all-mpnet-base-v2.raw.osf-bainbridge-2021-s2-0.human.feather")))

holdout_mapping_data = arrow::read_feather(
  file = file.path(data_path, glue("{model_name}.raw.osf-bainbridge-2021-s2-0.mapping2.feather"))
) %>%
  rename(scale_0 = scale0,
         scale_1 = scale1)

scales <- arrow::read_feather(file.path(data_path, glue("{model_name}.raw.osf-bainbridge-2021-s2-0.scales.feather"))
)

holdout_llm <- holdout %>%
  left_join(holdout_mapping_data %>% select(variable_1 = variable, InstrumentA = instrument, ScaleA = scale_0, SubscaleA = scale_1)) %>%
  left_join(holdout_mapping_data %>% select(variable_2 = variable, InstrumentA = instrument, ScaleA = scale_0, SubscaleA = scale_1))
```


## Precision simulation for synthetic reliabilities
```{r}
cors_real <- holdout_llm %>%
  select(x = variable_1, y = variable_2, r = empirical_r) %>%
  as.data.frame() |>
  igraph::graph_from_data_frame(directed = FALSE) |>
  igraph::as_adjacency_matrix(attr = "r", sparse = FALSE)
diag(cors_real) <- 1

mapping_data <- holdout_mapping_data
items_by_scale <- bind_rows(
  scales %>% filter(scale_1 == "") %>% left_join(mapping_data %>% select(-scale_1), by = c("instrument", "scale_0")),
  scales %>% filter(scale_1 != "") %>% left_join(mapping_data, by = c("instrument", "scale_0", "scale_1"))
)
  
n_distinct(scales$scale)

scales <- items_by_scale %>%
  group_by(keyed, scale) %>%
  summarise(
    items = list(variable),
    number_of_items = n_distinct(variable)) %>%
  drop_na() %>% 
  ungroup()

random_scales <- list()
for(i in 1:100000) {
  n_items <- rpois(1, rnorm(1, mean = 24, sd = 6))
  n_items <- if_else(n_items < 3, 3, n_items, 3)
  random_scales[[i]] <- holdout_mapping_data %>%
    sample_n(n_items) %>%
    mutate(scale = paste0("random", i)) %>%
    group_by(scale) %>%
    summarise(
      items = list(variable),
      number_of_items = n_distinct(variable)) %>%
    drop_na() %>% 
    mutate(keyed = 1)
}
names(random_scales) <- 1:length(random_scales) %>% as.character()
random_scales_df <- data.table::rbindlist(random_scales %>% map(as.data.frame)) %>% as_tibble()
scales_to_check <- bind_rows(scales, random_scales_df)
n_distinct(scales_to_check$scale)

source("../../../items_llm/global_functions.R")

scales_to_check <- scales_to_check %>% filter(number_of_items >= 3)

N <- 100

optimize_alpha_sa <- function(data, min_alpha = 0.70, temp = 1.0, cooling_rate = 0.95) {
  require(psych)
  
  current_data <- data
  best_data <- data
  best_alpha <- alpha(data, check.keys = TRUE)$total$raw_alpha
  
  repeat {
    alpha_results <- alpha(current_data, check.keys = TRUE)
    current_alpha <- alpha_results$total$raw_alpha
    
    # Cooling down the temperature
    temp <- temp * cooling_rate
    
    # Check if current alpha is good enough
    if (current_alpha >= min_alpha) {
      message("No further items need to be removed to reach ", min_alpha, ".\n", sep = "")
      break
    }
    
    # Using alpha.drop to see the impact of dropping each item
    alpha_drop <- alpha_results$alpha.drop
    if (is.null(alpha_drop) || nrow(alpha_drop) < 4) {
      message("No more items to drop. Final alpha is: ", current_alpha, "\n", sep = "")
      break
    }
    
    # Picking an item to drop, potentially accepting worse moves
    if (max(alpha_drop$raw_alpha) > best_alpha || runif(1) < exp((max(alpha_drop$raw_alpha) - best_alpha) / temp)) {
      best_item <- colnames(current_data)[which.max(alpha_drop$raw_alpha)]
      current_data <- current_data[, !colnames(current_data) %in% best_item, drop = FALSE]
      best_alpha <- max(alpha_drop$raw_alpha)
      message("Dropping item: ", best_item, " improved/worsened alpha to: ", best_alpha, "\n", sep = "")
    } else {
      message("Temperature too low to accept worse move. Stopping optimization.\n")
      break
    }
  }
  
  message("Optimization complete. Best alpha: ", best_alpha, ". Remaining items: ", paste(colnames(current_data), collapse = ", "), "\n")
  return(tibble(items = list(colnames(current_data)), alpha = best_alpha))
}



explore <- sample(1:nrow(bainbridge), size = N)
confirm <- setdiff(1:nrow(bainbridge), explore)
sample <- bainbridge %>% slice(explore)
confirm_sample <- bainbridge %>% slice(confirm)

set.seed(1)
scales_checked <- scales_to_check %>%
  rowwise() %>%
  mutate(
    rel_explore = suppressMessages(optimize_alpha_sa(sample[, items])),
    items_dropped = length(setdiff(items, unlist(rel_explore$items)))) %>% 
  mutate(rel_confirm = psych::alpha(confirm_sample[, unlist(rel_explore$items)], check.keys = T)$total$raw_alpha)

mean(scales_checked$rel_explore$alpha)
mean(scales_checked$rel_confirm)
mean(scales_checked$rel_explore$alpha[str_detect(scales_checked$scale, "random")])
mean(scales_checked$rel_confirm[str_detect(scales_checked$scale, "random")])
mean(scales_checked$rel_explore$alpha[scales_checked$rel_explore$alpha >= 0.7])
mean(scales_checked$rel_confirm[scales_checked$rel_explore$alpha >= 0.7])

scales_checked_r <- scales_checked[str_detect(scales_checked$scale, "random"), ]
mean(scales_checked_r$rel_explore$alpha[scales_checked_r$rel_explore$alpha >= 0.7])
mean(scales_checked_r$rel_confirm[scales_checked_r$rel_explore$alpha >= 0.7])


mean(scales_checked$rel_explore$alpha - scales_checked$rel_confirm)
cor(scales_checked$rel_explore$alpha - scales_checked$rel_confirm, scales_checked$items_dropped)
#%>%
  # mutate(
  #   alpha_se = mean(diff(unlist(psychometric::alpha.CI(rel_real_alpha, k = number_of_items, N = planned_N, level = 0.95))))
  # )

ggplot(scales_checked %>% filter(str_detect(scale, "random")), aes(rel_explore$alpha)) + geom_histogram(binwidth = 0.01)
ggplot(scales_checked %>% filter(str_detect(scale, "random")), aes(rel_confirm)) + geom_histogram(binwidth = 0.01) # , fill = round(rel_confirm, 2) == 0.7)

ggplot(scales_checked, aes(rel_explore$alpha, rel_confirm)) + 
  geom_point() + 
  geom_abline()

```